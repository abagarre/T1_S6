<h5 style="text-align: center"> STATISTIQUES </h5>

------

## **Modélisation statistique des données**

------

### Rappels

- Loi courantes :
  - **Bernoulli** : $\mathcal B(p), \ p\in [0,1]​$, de densité : $f(x) = p^x(1-p)^{1-x} \ \pmb 1_{\{0,1\}}(x)​$
  - **Uniforme** : $\mathcal U([a,b]), \ a<b$, de densité : $f(x) = \cfrac{1}{b-a} \ \pmb 1_{[a,b]}(x) $
  - **Normale** : $\mathcal N(\mu, \sigma^2), \ \mu \in \R​$, de densité : $f(x)=\frac{1}{\sigma\sqrt{2\pi}}\cdot \exp{\left( - \frac{(x-\mu)^2}{2\sigma^2} \right)}​$
  - **Normale multivariée** : $\mathcal N_{\R^n}(\pmb \mu, \mathbf R), \ \pmb \mu \in \R^n, \ \mathbf R$ matrice de covariance définie positive, de densité $f(\mathbf x)=\sqrt{\frac{1}{(2\pi)^n \det(\mathbf R)}} \cdot \exp{\left(-\frac{1}{2} (\mathbf x - \pmb \mu)^T \mathbf R^{-1} (\mathbf x -  \pmb \mu) \right)}$
  - **Exponentielle** : $\mathcal E(\theta), \ \theta>0$, de densité $f(x)=\theta e^{-\theta x} \ \pmb 1_{\R^+}(x)$
  - **Khi deux** : $\chi^2(n)$ telle que $Y \sim \chi^2(n)$ si $Y=\Vert \mathbf X\Vert^2$ avec $\mathbf X \sim \mathcal N_{\R^n}(\pmb 0, \mathbf I_n)$, de densité $f(x)=\cfrac{x^{n/2 \ - 1} \exp{(-x/2)}}{2^{n/2} \Gamma(n/2)} \ \pmb 1_{\R^+}(x)$  où $\Gamma (u) = \int_{\R^+} t^{u-1} \exp{(-t)}dt \ \Big(\Gamma(u) = (u-1)! \text{ si } u \in \N^* \Big)$

---

### Modèle statistique

| <u>**Observation**</u>                                       |
| :----------------------------------------------------------- |
| Une **observation** de $X$ est une variable aléatoire à valeurs dans un ensemble $\mathcal X$, dit *ensemble d’observation* |

- Dans le cours, souvent $\mathcal X \subset \R^p$ et dans la plus part des exemples, $p=1$.

| <u>**Modèle statistique**</u>                                |
| :----------------------------------------------------------- |
| Modèle statistique sur $\mathcal X$ : famille de lois discrètes (resp. absolument continues) représentées par leur fonction de masse (resp. densité) $\mathcal M=\{f_\theta : \theta \in \Theta\}$ et paramétré par un ensemble $\Theta$ |

| <u>**Identifiabilité**</u>                                   |
| :----------------------------------------------------------- |
| Un modèle statistique est **identifiable** si $\theta \mapsto f_\theta$ est injective |

---

### Échantillons et statistiques

| <u>**Échantillon**</u>                                       |
| :----------------------------------------------------------- |
| $n-$échantillon : séquence $X_1,\cdots,X_n$ d’observations indépendantes et identiquement distribuées de loi individuelle $f_\theta$ |

| **<u>Statistique</u>**                                       |
| :----------------------------------------------------------- |
| Variable aléatoire s’exprimant comme fonction $T$ d’un $n-$échantillon |

- **Moyenne empirique** : $T=\overline{X_n}=\frac{1}{n}\sum\limits_{k=1}^{n}X_k$
- **Variance empirique** : $T = \frac{1}{n-1}\sum\limits_{k=1}^n \left(X_k-\overline{X_n}\right)^2$

| **<u>Exhaustivité</u>**                                      |
| :----------------------------------------------------------- |
| Une statistique $T$ est **exhaustive** (ou suffisante) si la loi conditionnelle $\mathcal L(X_1,\cdots,X_n \mid T=t )​$ est indépendante de $\theta​$ |

- Les statistiques exhaustives sont celles qui n’entraînent pas de perte d’information sur le paramètre $\theta$

- **Théorème de factorisation de Fisher** : 

  - $T$ est exhaustive $\iff \prod\limits_{k=1}^n f_\theta (x_k) = g_\theta(T(x_1, \cdots,x_n))\cdot h(x_1,\cdots, x_n)$ 

    où $g_\theta$ est une fonction positive dépendant de $\theta$ et $h$ est une fonction positive indépendante de $\theta$

---

### Modélisation bayésienne

| **<u>Modèle bayésien</u>**                                   |
| :----------------------------------------------------------- |
| Donnée d’un modèle statistique $\mathcal M = \{f_\theta : \theta \in \Theta\}$ et d’une **loi *a priori*** $\pi$ sur $\Theta$ |

- Le paramètre $\theta$ est maintenant aléatoire, suivant une loi *a priori*
- Dans le contexte bayésien, la loi d’une observation $\mathcal L(X)$ est appelée **évidence** et la loi $\mathcal L (\theta \mid X=x)$ est appelée **loi *a posteriori***



------

<table width="90%">
<tr>
<td style="width: 30%; text-align: left; background:transparent; border:0;">Information et Statistiques</td>
<td style="width: 30%; text-align: center; background:transparent; border:0;">Alexis Bagarre</td>
<td style="width: 30%; text-align: right; background:transparent; border:0;">T1 - 2018 / 2019</td>
</tr>
</table>